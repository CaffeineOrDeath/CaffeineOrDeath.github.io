---
slug: google-data-analytics
title: Food Import Cost Case Study
date: 2023-11-12
lastmod: ["lastmod", ":git", "date", "publishDate"]
tags: ["R", "ggplot", "python", "statistics", "matplotlib"]
---

This topic was chosen as there is a lot that goes into the production of food. A level of concern was raised as it became obvious that people with various disorders are struggling to get the food items they can have. By this it is meant, those with Crohn's disease, restrictive diets due to genetics (dysautonomia, EDS, ANS, etc), as well as many others. Medications can make the lack of proper foods tolerable, but it does not solve the overall issue they have.
<!--more-->

# Week 1

## Setup
```{r setup, include=FALSE}
library(knitr)
knitr::opts_knit$set(root.dir = ".")
```
The libraries in use:
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(plotly)
library(readxl)
```
## Planning Stage

1)  Find the data set(s) for food production costs (first place to look is USDA)\
2)  Find the data set(s) for consumer cost of the items (USDA?)\
3)  Start plotting the costs against each other (Smooth vs Bar charts)\
4)  Figure out the pattern in the data (likely going to have go back over the course of a few years (on returning, apparently decades))

The main question, why does food cost so much? To answer this question, we need a few bits of data to start with. Those data sets are, the current cost to produce (e.g. grow fruits) and/or manufacture (e.g. process grains) the food items. From there, we need to get a data set for the cost to consumers of these same items. Considering that I live in the United States, I'll be working with those data sets first.

## Let's begin

Time to start gathering the data sets. Charts will appear in the order made in the order of the data sets. If more data sets are listed than there are charts, then the initial exploration of the data is not done.

Well, we have two charts made showing the breakdown of imported goods and the year-over-year change in cost. The spikes in import price or the amount of food still needed to be imported raises some concern. Further review will be needed to provide, not only context, but an explanation as to why we saw such a sharp increase.

# End of Week 1 Update

The data is immense. As of right now, the data still requires some restructuring for it to be more accessible for visualization. The Vegetables and Pulses Yearbook, while extensive (50+ tables), is structured for spreadsheet use. Have a happy holiday!

# Week 2

The line chart is the average of imported vegetables and pulses available by US pounds per person. It was made using ggplot2 and a custom data frame.

# End of Week 2

Upon further review, there are a lot of missing pieces. Namely, trade agreements. While we have import costs, this would be offset by trades with respective countries. For example, exporting wheat to import avocados. 

# Data sets

[Food Imports](https://www.ers.usda.gov/data-products/u-s-food-imports/)\
[Vegetables and Pulses Yearbook](https://www.ers.usda.gov/data-products/vegetables-and-pulses-data/vegetables-and-pulses-yearbook-tables/)\
[Farm Output](https://www.ers.usda.gov/data-products/agricultural-productivity-in-the-u-s/)\
[Dairy](https://www.ers.usda.gov/data-products/dairy-data.aspx)

# Charts

![2022 US Imports (Spreadsheet)](img/2022-US-Imports.png)

![US Imported Food YoY (Spreadsheet)](img/US-Imported-Food-Price-Change.png)

Of course, we need some R plots. Sheet 3 is "U.S. per capita availability of selected, commercially produced, fresh and processing vegetables and dry pulse crops".
```{r echo=TRUE, out.height=400, out.width=500, message=FALSE, warning=FALSE}
pc <- read_xlsx("datasets/VPPT.xlsx", sheet = 3)
avg <- setNames(aggregate(pc$Value, list(pc$Year), mean), c("year", "value"))


p <- ggplot(avg, aes(year, value)) +
  geom_line() +
  labs(title = "Average Vegetables and Pulses Per Person") +
  ylab("Weight per Person (lbs)") + xlab("Year")

ggplotly(p)

```
(interactive ggplot with custom data frame using the second data set, sheet 3)

Now for some python fun. Let's begin by loading R's python package.
```{r message=FALSE, warning=FALSE, results='hide'}
# if it is not already installed, run the following command
# install.packages('reticulate', repos="http://cran.us.r-project.org")
library(reticulate)
```
Load python's respective packages.
```{python}
# if they are not installed, run the following command
# pip install pandas matplotlib
import pandas as pd
import matplotlib.pyplot as plt
```
While the last sheet used contained vegetables, this sheet contains pulses. For this instance, the sheet we are using are is sheet 6, Dry Pulse Crops: U.S. per capita availability. The header row is row 1 (zero indexing in python versus one indexing in sheets) so we need to specify this in the header argument. Executing the following commands should give us a clean data frame.
```{python}
df = pd.read_excel('datasets/VPPT.xlsx', sheet_name=6, header=1)
df.info()
```
Well, this is interesting. If we look back on the data source, we can see how the "Year" column ended up with 58 values. There are additional notes at the bottom of the sheet. Also, we will notice some null values for a few columns. Namely the blackeye, garbanzo, pink, red kidney (dark), red kidney (light) and small red columns. The maximum number of null rows is 20(!) in both red kidney columns however, red kidney (all) has all rows.

Additionally, the notes state that the total is the sum of the other columns, respectively. In this case, we can ignore many of the columns for now. Time to extract the necessary data.

First, we will extract the Year and all dry edible beans. Preliminary glance at the data shows values dropping. 

While trying to access the Year column, pandas kept throwing "KeyError: 'Year'". Something not noticed in the sheet... There is a trailing whitespace on the column name.
```{python}
df.rename(columns=lambda x: x.strip(), inplace=True)
df.keys()
df = df.dropna()
```
```{python}
all_years=df['Year']
all_dry_edible=df['All dry edible beans']
```

Time to see it in action.
```{python}
plt.bar(all_years, all_dry_edible)
plt.title('All Dry Edible Beans Per Capita')
plt.ylabel('lbs')
plt.xlabel('year')
plt.show()
```

## Additional Questions

1.  Why did 2008 and 2013 have the largest percentage change?
2.  In the line chart, we see a steep drop at 1980, what happened?
3.  In the line chart, we see a spike at 2020, what happened? 
